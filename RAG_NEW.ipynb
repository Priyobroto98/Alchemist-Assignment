{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in c:\\users\\user\\anaconda3\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: langchain in c:\\users\\user\\anaconda3\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: langchain_cohere in c:\\users\\user\\anaconda3\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: langchain_groq in c:\\users\\user\\anaconda3\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<2.0.36,>=1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (0.5.14)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (0.3.19)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (0.1.134)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (2.5.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: cohere<6.0,>=5.5.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_cohere) (5.11.0)\n",
      "Requirement already satisfied: langchain-experimental>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_cohere) (0.3.3)\n",
      "Requirement already satisfied: pandas>=1.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_cohere) (2.2.3)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_cohere) (0.9.0)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_groq) (0.11.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.3)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.34.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (1.35.38)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (1.9.7)\n",
      "Requirement already satisfied: httpx>=0.21.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (0.27.2)\n",
      "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (0.9.0)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (2.23.4)\n",
      "Requirement already satisfied: sagemaker<3.0.0,>=2.232.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (2.232.2)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (0.20.3)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (2.32.0.20240914)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (4.12.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.8.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.17->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.17->langchain_community) (24.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.4.3->langchain_cohere) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.4.3->langchain_cohere) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.4.3->langchain_cohere) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from SQLAlchemy<2.0.36,>=1.4->langchain_community) (3.0.1)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.38 in c:\\users\\user\\anaconda3\\lib\\site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain_cohere) (1.35.38)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain_cohere) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain_cohere) (0.10.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain_cohere) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain_cohere) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.17->langchain_community) (2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.4.3->langchain_cohere) (1.16.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (2.2.1)\n",
      "Requirement already satisfied: docker in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (7.1.0)\n",
      "Requirement already satisfied: google-pasta in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (6.11.0)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (4.19.2)\n",
      "Requirement already satisfied: pathos in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.3.3)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (4.3.6)\n",
      "Collecting protobuf<5.0,>=3.12 (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere)\n",
      "  Using cached protobuf-4.25.5-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (5.9.0)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.0.10)\n",
      "Requirement already satisfied: sagemaker-mlflow in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.1.0)\n",
      "Requirement already satisfied: schema in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.7.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (4.67.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain_cohere) (0.26.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain_cohere) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain_cohere) (2024.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.21.0)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (13.9.4)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.10.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.4.6)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\user\\anaconda3\\lib\\site-packages (from docker->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (305.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.7.6.9)\n",
      "Requirement already satisfied: dill>=0.3.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.3.9)\n",
      "Requirement already satisfied: pox>=0.3.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.3.5)\n",
      "Requirement already satisfied: multiprocess>=0.70.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.70.17)\n",
      "Requirement already satisfied: mlflow>=2.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (2.16.2)\n",
      "Requirement already satisfied: mlflow-skinny==2.16.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (2.16.2)\n",
      "Requirement already satisfied: Flask<4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (2.3.2)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.13.3)\n",
      "Requirement already satisfied: graphene<4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.3)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.4.1)\n",
      "Requirement already satisfied: matplotlib<4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.8.0)\n",
      "Requirement already satisfied: pyarrow<18,>=4.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (14.0.2)\n",
      "Requirement already satisfied: scikit-learn<2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.5.2)\n",
      "Requirement already satisfied: scipy<2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.13.1)\n",
      "Requirement already satisfied: Jinja2<4,>=3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.1.3)\n",
      "Requirement already satisfied: waitress<4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.0.0)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (8.1.7)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.34.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.1.37)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.28.1)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.28.1)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.5.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (2.18.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\user\\anaconda3\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.3.5)\n",
      "Requirement already satisfied: Werkzeug>=2.3.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.1.1)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.6.2)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.2.4)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.2.0)\n",
      "Requirement already satisfied: aniso8601<10,>=8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (9.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Jinja2<4,>=3.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (2.1.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.0.9)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.5.0)\n",
      "Requirement already satisfied: google-auth~=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (2.36.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (4.0.7)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.2.15)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.49b1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.16.0)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (4.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.6.1)\n",
      "Using cached protobuf-4.25.5-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.3\n",
      "    Uninstalling protobuf-5.28.3:\n",
      "      Successfully uninstalled protobuf-5.28.3\n",
      "Successfully installed protobuf-4.25.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-proto 1.28.1 requires protobuf<6.0,>=5.0, but you have protobuf 4.25.5 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_community langchain langchain_cohere langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\user\\anaconda3\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain) (0.3.19)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain) (0.1.134)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_groq import ChatGroq  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "GROQ_API_KEY='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'\n",
    "os.environ[\"GROQ_API_KEY\"]='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm= ChatGroq(\n",
    "            model='llama3-8b-8192',\n",
    "            temperature=0.6,\n",
    "            api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Programming! It\\'s like trying to explain a joke to a computer. You have to be precise, or it just won\\'t get it!\\n\\nBut seriously, programming is like building with Legos, but instead of blocks, you use code! You start with a foundation, add some logic, and then... voilÃ ! You create something amazing!\\n\\nHere\\'s a joke to illustrate the process:\\n\\nWhy did the programmer quit his job?\\n\\nBecause he didn\\'t get arrays! (get a raise)\\n\\nOkay, maybe it\\'s a bit of a \"bug\" (get it?), but I hope it \"compiles\" a smile on your face!\\n\\nNow, if you\\'re interested in learning more about programming, I\\'d be happy to help you \"debug\" any questions you might have!' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 158, 'prompt_tokens': 28, 'total_tokens': 186, 'completion_time': 0.131666667, 'prompt_time': 0.003600131, 'queue_time': 0.009942199, 'total_time': 0.135266798}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None} id='run-a435d36b-5869-4a82-958f-d82ad7e9e973-0' usage_metadata={'input_tokens': 28, 'output_tokens': 158, 'total_tokens': 186}\n",
      "content='Programming!\\n\\nWhy did the programmer quit his job?\\n\\nBecause he didn\\'t get arrays! (get a raise)\\n\\nBut seriously, programming is a fascinating field that involves designing, writing, testing, and maintaining the source code of computer programs. It\\'s a crucial part of many industries, from video games and mobile apps to operating systems and artificial intelligence.\\n\\nHere are some programming basics:\\n\\n1. Variables: Like labeled boxes that store values, such as numbers or words.\\n2. Loops: Repeating a set of instructions until a condition is met, like a never-ending loop of coffee refills.\\n3. Conditional statements: If-else statements that help your program make decisions, like \"if it\\'s Friday, go to the beach!\"\\n4. Functions: Reusable blocks of code that perform a specific task, like a \"hello world\" function.\\n5. Debugging: Finding and fixing errors in your code, like chasing a pesky bug.\\n\\nWhy did the programmer go to the doctor?\\n\\nBecause he was feeling a little glitchy!\\n\\nProgramming is an amazing skill to learn, and with practice, you can create incredible things. From simple scripts to complex algorithms, the possibilities are endless!\\n\\nWant to learn more about programming? I can help you get started with some fun and easy tutorials!\\n\\nWhat do you think? Are you ready to start coding?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 271, 'prompt_tokens': 28, 'total_tokens': 299, 'completion_time': 0.225833333, 'prompt_time': 0.003597752, 'queue_time': 0.016753795000000002, 'total_time': 0.229431085}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None} id='run-657ef705-ef0a-44f7-9a9e-1fe3516b379c-0' usage_metadata={'input_tokens': 28, 'output_tokens': 271, 'total_tokens': 299}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that tells jokes.\"),\n",
    "    HumanMessage(content=\"Tell me about programming\")\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "print(response)\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant that tells jokes.\"),\n",
    "    (\"human\", \"Tell me about {topic}\")\n",
    "])\n",
    "chain = template | llm\n",
    "response = chain.invoke({\"topic\": \"programming\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doccument Processing For RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents from the folder.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "def load_documents(folder_path: str) -> List[Document]:\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif filename.endswith('.docx'):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {filename}\")\n",
    "            continue\n",
    "        documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "folder_path = r\"C:\\Users\\user\\Desktop\\Alchemist AI Assignment\\data\"\n",
    "documents = load_documents(folder_path)\n",
    "print(f\"Loaded {len(documents)} documents from the folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the documents into 115 chunks.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Split the documents into {len(splits)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='dynasty, ruled over the Indian subcontinent from' metadata={'source': 'C:\\\\Users\\\\user\\\\Desktop\\\\Alchemist AI Assignment\\\\data\\\\The Mughal Empire.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(splits[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\user\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "embeddings_model_name='embed-english-v2.0'\n",
    "COHERE_API_KEY='30lAzH7VO1y9M00rSD5ns4gIIxicqxiMnCn1IycB'\n",
    "import os\n",
    "os.environ['COHERE_API_KEY']=COHERE_API_KEY \n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "embeddings = CohereEmbeddings(\n",
    "        model=embeddings_model_name,\n",
    "        cohere_api_key=os.getenv('COHERE_API_KEY')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CohereEmbeddings(client=<cohere.client.Client object at 0x0000020B84C4EF50>, async_client=<cohere.client.AsyncClient object at 0x0000020BE68C4450>, model='embed-english-v2.0', truncate=None, cohere_api_key=SecretStr('**********'), embedding_types=['float'], max_retries=3, request_timeout=None, user_agent='langchain:partner', base_url=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypika.dialects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections: []\n"
     ]
    }
   ],
   "source": [
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "\n",
    "# Initialize ChromaDB with persistence\n",
    "settings = Settings(persist_directory=\"./chroma_db\")\n",
    "client = Client(settings=settings)\n",
    "\n",
    "# List available collections\n",
    "try:\n",
    "    collections = client.list_collections()\n",
    "    print(f\"Collections: {collections}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error listing collections: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection created: Collection(name=my_collection)\n"
     ]
    }
   ],
   "source": [
    "# Create or get a collection\n",
    "try:\n",
    "    collection = client.get_or_create_collection(name=\"my_collection\")\n",
    "    print(\"Collection created:\", collection)\n",
    "except Exception as e:\n",
    "    print(f\"Error creating collection: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created and persisted to './chroma_db'\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "collection_name=\"my_collection\"\n",
    "vectorstore = Chroma.from_documents(\n",
    "    collection_name=collection_name,\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "print(\"Vector store created and persisted to './chroma_db'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 2 most relevant chunks for the query: 'What was babar?'\n",
      "\n",
      "Result 1:\n",
      "Source: C:\\Users\\user\\Desktop\\Medical ChatBot\\data\\The Mughal Empire.pdf\n",
      "Content: culture with the rich traditions of the Indian subcontinent, resulting in a unique and lasting legacy in art, \n",
      "architecture, and administration. \n",
      "Babur's grandson, Akbar the Great, is often credited with consolidating and expanding the Mughal \n",
      "Empire. Ascending the throne at the tender age of 13, Akbar displayed exceptional military prowess and \n",
      "strategic acumen. His policy of religious tolerance and inclusion, known as Sulh-e-Kul or \"universal\n",
      "\n",
      "Result 2:\n",
      "Source: C:\\Users\\user\\Desktop\\Medical ChatBot\\data\\The Mughal Empire.pdf\n",
      "Content: culture with the rich traditions of the Indian subcontinent, resulting in a unique and lasting legacy in art, \n",
      "architecture, and administration. \n",
      "Babur's grandson, Akbar the Great, is often credited with consolidating and expanding the Mughal \n",
      "Empire. Ascending the throne at the tender age of 13, Akbar displayed exceptional military prowess and \n",
      "strategic acumen. His policy of religious tolerance and inclusion, known as Sulh-e-Kul or \"universal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What was babar?\"\n",
    "search_results = vectorstore.similarity_search(query, k=2)\n",
    "print(f\"\\nTop 2 most relevant chunks for the query: '{query}'\\n\")\n",
    "for i, result in enumerate(search_results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"Source: {result.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Content: {result.page_content}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "# retriever_results = retriever.invoke(\"Who was Akbar?\")\n",
    "# print(retriever_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def docs2str(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was Akbar?\n",
      "Answer: Akbar was Babur's grandson.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who was Akbar?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was his son?\n",
      "Answer: Answer: Humayun\n"
     ]
    }
   ],
   "source": [
    "question = \"Who was his son?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was his religious policy?\n",
      "Answer: The question is asking for information based on the provided context, which only mentions \"strategic acumen\" and \"policy of religious\". Therefore, the answer would be \"not stated\" or \"unknown\", as there is no specific information provided about his religious policy in the given context.\n"
     ]
    }
   ],
   "source": [
    "question = \"What was his religious policy?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**History Aware Retriver**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "contextualize_q_system_prompt = \"\"\"\n",
    "Given a chat history and the latest user question\n",
    "which might reference context in the chat history,\n",
    "formulate a standalone question which can be understood\n",
    "without the chat history. Do NOT answer the question,\n",
    "just reformulate it if needed and otherwise return it as is.\n",
    "\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "contextualize_chain = contextualize_q_prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who was Akbar the Great?\n"
     ]
    }
   ],
   "source": [
    "print(contextualize_chain.invoke({\"input\": \"Who was Akbar?\", \"chat_history\": []}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use the following context to answer the user's question.Answer to the point and precise manner.\"),\n",
    "    (\"system\", \"Context: {context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**History Aware RAG chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Who was Akbar?\n",
      "AI: Akbar the Great was the third Mughal Emperor of India, ruling from 1556 to 1605. He was the grandson of Babur, the founder of the Mughal Empire.\n",
      "\n",
      "Human: Who was his Son?\n",
      "AI: Akbar's son was Jahangir, who succeeded him as the fourth Mughal Emperor of India.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = []\n",
    "question1 = \"Who was Akbar?\"\n",
    "answer1 = rag_chain.invoke({\"input\": question1, \"chat_history\": chat_history})['answer']\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question1),\n",
    "    AIMessage(content=answer1)\n",
    "])\n",
    "\n",
    "print(f\"Human: {question1}\")\n",
    "print(f\"AI: {answer1}\\n\")\n",
    "\n",
    "question2 = \"Who was his Son?\"\n",
    "answer2 = rag_chain.invoke({\"input\": question2, \"chat_history\": chat_history})['answer']\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question2),\n",
    "    AIMessage(content=answer2)\n",
    "])\n",
    "\n",
    "print(f\"Human: {question2}\")\n",
    "print(f\"AI: {answer2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining Every Block of Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Constants\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n",
    "model_type = os.getenv('MODEL_TYPE')\n",
    "\n",
    "\n",
    "# Initialize LLM and Embeddings\n",
    "llm = ChatGroq(\n",
    "    model=model_type,\n",
    "    temperature=0.7,\n",
    "    api_key=os.getenv('GROQ_API_KEY')\n",
    ")\n",
    "\n",
    "def create_chains(llm, retriever):\n",
    "    \"\"\"Create necessary chains for the RAG pipeline.\"\"\"\n",
    "    contextualize_ques_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        Given a chat history and the latest user question\n",
    "        which might reference context in the chat history,\n",
    "        formulate a standalone question which can be understood\n",
    "        without the chat history. Do NOT answer the question,\n",
    "        just reformulate it if needed and otherwise return it as is.\n",
    "        \"\"\"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    contextualize_chain = contextualize_ques_prompt | llm | StrOutputParser()\n",
    "    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_ques_prompt)\n",
    "\n",
    "    qa_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        You are a helpful RAG AI assistant. \n",
    "        Use the following context to answer the user's question. \n",
    "        Answer to the point,precise and in a very specific manner.\n",
    "        If it is an out-of-context question, do not answer and say it is out of context.\n",
    "        \"\"\"),\n",
    "        (\"system\", \"Context: {context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    return rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_14884\\1589971558.py:18: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  generic_chain = LLMChain(llm=llm, prompt=generic_prompt)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class RouterChain with abstract methods _call, input_keys",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 52\u001b[0m\n\u001b[0;32m     41\u001b[0m     router \u001b[38;5;241m=\u001b[39m RouterChain(\n\u001b[0;32m     42\u001b[0m         router_chain\u001b[38;5;241m=\u001b[39mrouter_chain,\n\u001b[0;32m     43\u001b[0m         destination_chains\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m         default_chain\u001b[38;5;241m=\u001b[39mgeneric_chain,  \u001b[38;5;66;03m# Fallback to generic if classification fails\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     )\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m router\n\u001b[1;32m---> 52\u001b[0m router_chain \u001b[38;5;241m=\u001b[39m create_router_chain(llm, retriever)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Test input\u001b[39;00m\n\u001b[0;32m     55\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me something interesting.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[21], line 41\u001b[0m, in \u001b[0;36mcreate_router_chain\u001b[1;34m(llm, retriever)\u001b[0m\n\u001b[0;32m     38\u001b[0m router_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39mrouter_prompt)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Combine chains with RouterChain\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m router \u001b[38;5;241m=\u001b[39m RouterChain(\n\u001b[0;32m     42\u001b[0m     router_chain\u001b[38;5;241m=\u001b[39mrouter_chain,\n\u001b[0;32m     43\u001b[0m     destination_chains\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecific\u001b[39m\u001b[38;5;124m\"\u001b[39m: rag_chain,\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneric\u001b[39m\u001b[38;5;124m\"\u001b[39m: generic_chain,\n\u001b[0;32m     46\u001b[0m     },\n\u001b[0;32m     47\u001b[0m     default_chain\u001b[38;5;241m=\u001b[39mgeneric_chain,  \u001b[38;5;66;03m# Fallback to generic if classification fails\u001b[39;00m\n\u001b[0;32m     48\u001b[0m )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m router\n",
      "\u001b[1;31mTypeError\u001b[0m: Can't instantiate abstract class RouterChain with abstract methods _call, input_keys"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, MultiRouteChain\n",
    "from langchain.chains.router import RouterChain\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def create_generic_prompt_chain(llm):\n",
    "    \"\"\"Create a chain to handle vague/generic inputs.\"\"\"\n",
    "    generic_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        The user's input seems vague or generic. Your task is to guide the user\n",
    "        to provide a more specific input. Do NOT answer the query; instead, \n",
    "        politely ask for clarification or specificity.\n",
    "        \"\"\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    generic_chain = LLMChain(llm=llm, prompt=generic_prompt)\n",
    "    return generic_chain\n",
    "\n",
    "def create_router_chain(llm, retriever):\n",
    "    \"\"\"Combine RAG chain and generic handling chain into a RouterChain.\"\"\"\n",
    "    # Create the history-aware RAG chain\n",
    "    rag_chain = create_chains(llm, retriever)\n",
    "\n",
    "    # Create the generic prompt chain\n",
    "    generic_chain = create_generic_prompt_chain(llm)\n",
    "\n",
    "    # Define the router logic\n",
    "    router_prompt = PromptTemplate(\n",
    "        input_variables=[\"input\"],\n",
    "        template=\"\"\"\n",
    "        Decide whether the following input is 'specific' or 'generic':\n",
    "        Input: {input}\n",
    "        Respond with 'specific' or 'generic' only.\n",
    "        \"\"\"\n",
    "    )\n",
    "    router_chain = LLMChain(llm=llm, prompt=router_prompt)\n",
    "\n",
    "    # Combine chains with RouterChain\n",
    "    router = RouterChain(\n",
    "        router_chain=router_chain,\n",
    "        destination_chains={\n",
    "            \"specific\": rag_chain,\n",
    "            \"generic\": generic_chain,\n",
    "        },\n",
    "        default_chain=generic_chain,  # Fallback to generic if classification fails\n",
    "    )\n",
    "    return router\n",
    "\n",
    "\n",
    "router_chain = create_router_chain(llm, retriever)\n",
    "\n",
    "# Test input\n",
    "user_input = \"Tell me something interesting.\"\n",
    "response = router_chain.run({\"input\": user_input})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "def create_generic_prompt_chain(llm):\n",
    "    \"\"\"Create a chain to handle vague/generic inputs.\"\"\"\n",
    "    generic_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        The user's input seems vague or generic. Your task is to guide the user\n",
    "        to provide a more specific input. Do NOT answer the query; instead, \n",
    "        politely ask for clarification or specificity.\n",
    "        \"\"\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    generic_chain = LLMChain(llm=llm, prompt=generic_prompt)\n",
    "    return generic_chain\n",
    "\n",
    "def router_logic(llm, user_input):\n",
    "    \"\"\"Decides if the input is 'specific' or 'generic'.\"\"\"\n",
    "    router_prompt = PromptTemplate(\n",
    "        input_variables=[\"input\"],\n",
    "        template=\"\"\"\n",
    "        Based on the input provided, classify it as either 'specific' or 'generic':\n",
    "        Input: {input}\n",
    "        Respond with either 'specific' or 'generic' only.\n",
    "        \"\"\"\n",
    "    )\n",
    "    router_chain = LLMChain(llm=llm, prompt=router_prompt)\n",
    "    classification = router_chain.run({\"input\": user_input})\n",
    "    return classification.strip().lower()\n",
    "\n",
    "def create_router_chain(llm, retriever):\n",
    "    \"\"\"Combines RAG and generic chains with routing logic.\"\"\"\n",
    "    # Create the history-aware RAG chain\n",
    "    rag_chain = create_chains(llm, retriever)\n",
    "\n",
    "    # Create the generic chain\n",
    "    generic_chain = create_generic_prompt_chain(llm)\n",
    "\n",
    "    # Routing function\n",
    "    def route_input(user_input):\n",
    "        classification = router_logic(llm, user_input)\n",
    "        if classification == \"specific\":\n",
    "            return rag_chain.run({\"input\": user_input})\n",
    "        elif classification == \"generic\":\n",
    "            return generic_chain.run({\"input\": user_input})\n",
    "        else:\n",
    "            # Fallback for unclassified cases\n",
    "            return \"I'm sorry, I couldn't classify your input. Please try again.\"\n",
    "\n",
    "    return route_input\n",
    "\n",
    "\n",
    "router_chain = create_router_chain(llm, retriever)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'd love to share something interesting with you! However, I'd like to clarify what kind of interesting you're looking for.\\n\\nAre you interested in learning something new about a particular topic, such as science, history, or technology? Or perhaps you're looking for something more lighthearted, like a fun fact or a surprising statistic?\\n\\nCould you please give me a bit more context or guidance on what you'd find interesting? That way, I can provide a more tailored response that meets your curiosity!\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "user_input = \"Tell me something interesting.\"\n",
    "response = router_chain(user_input)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_router_chain(llm, retriever,user_input):\n",
    "    \"\"\"Combines RAG and generic chains with routing logic.\"\"\"\n",
    "    # Create the history-aware RAG chain\n",
    "    rag_chain = create_chains(llm, retriever)\n",
    "\n",
    "    # Create the generic chain\n",
    "    generic_chain = create_generic_prompt_chain(llm)\n",
    "\n",
    "    # Routing function\n",
    "    def route_input(user_input):\n",
    "        classification = router_logic(llm, user_input)\n",
    "        if classification == \"specific\":\n",
    "            return rag_chain.invoke({\n",
    "                \"input\": user_input\n",
    "                \n",
    "            })['answer']\n",
    "            \n",
    "        elif classification == \"generic\":\n",
    "            return generic_chain.run({\"input\": user_input})\n",
    "        else:\n",
    "            # Fallback for unclassified cases\n",
    "            return \"I'm sorry, I couldn't classify your input. Please try again.\"\n",
    "\n",
    "    return route_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.create_router_chain.<locals>.route_input(user_input)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "user_input = \"Tell me something interesting.\"\n",
    "response = create_router_chain(llm, retriever,user_input)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
